{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import packages\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport path\n\nimport torch\nimport torchvision\nfrom torch.utils import data\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\ndef prepare_labels(y):\n    # From here: https://www.kaggle.com/pestipeti/keras-cnn-starter\n    values = np.array(y)\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\n    y = onehot_encoded\n    return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normaly I do this with FastAI, but I want to challenge myself and do it in PyTorch to get deeper :)\n# For this cell, I'm borrowing and modifying code from good old forums. But I'll explain it to make sure I understand every line of code :)\n\n# First I'll define a custom dataset for my eye images. I can't use torchvision.Imagefolder because my lables are in a csv. \nclass eyeDataset(data.Dataset):\n    #initializing it, taking in the text and image path. With an optional transform\n    def __init__(self, txt_path, img_dir, transform=None):\n        #First define a pandas dataframe from the CSV. The ID code (aka name of the image) will be the index\n        df = pd.read_csv(txt_path, index_col=0)\n        #Define image and text paths\n        self.img_dir = img_dir\n        self.txt_path = txt_path\n        #The image names = the index of the dataframe\n        self.img_names = df.index.values\n        #The labels of the dataframe come from diagnosis\n        self.y = prepare_labels(df['diagnosis'].values)\n        #And the transforms\n        self.transform = transform\n    \n    #When we indext the dataset, it will run this functon. This function returns the image and the label\n    def __getitem__(self, index):\n        #Opens the image (Lesson: Put a plus to merge the paths together without creating a sub-branch)\n        img = Image.open(os.path.join(self.img_dir,\n                                      self.img_names[index] + \".png\"))\n        #Transforms the image if we want\n        if self.transform is not None:\n            img = self.transform(img)\n        #Labels it by accessing it from the dataframe\n        label = self.y[index]\n        #Returns image and label\n        return img, label\n\n    def __len__(self):\n        return self.y.shape[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining it\nlabel_path = \"../input/aptos2019-blindness-detection/train.csv\"\nimage_path = \"../input/aptos2019-blindness-detection/train_images\"\nds = eyeDataset(label_path, image_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Teting it out\nimg, lab = ds[0];img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking at the size\nsize = img.size\nsize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#That's huge! No way I'm going to train on that for the first iteration. I'll resize it down. But the good news is that we can do progressive resizing like crazy!\n\n#The type of transforms: Resize, centercrop, Random vertical and horizonal flips,\n\n#Order = I'll scale down, center crop it, then flips,\nratio = 0.10\n\ntransform_bunch = transforms.Compose([transforms.Resize([int(2136 * ratio), int(3216 * ratio)]),\n                                      transforms.CenterCrop([int(2136 * ratio),int(2136 * ratio)]),\n                                      transforms.RandomHorizontalFlip(p=0.5),\n                                      transforms.RandomVerticalFlip(p=0.5)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = eyeDataset(label_path, image_path, transform_bunch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img, label = ds[0]; img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #But we still need to convert to tensor... But while we're doing that, we might as well normalize the data.\n\n# #When getting the data for mean and std, we must do it only on the train images because if we did it for the test images it would polute our numbers\n\n# Shamelessly just borrowed the code from online. But I'll only be running it once to get the values\n\n# transform_bunch = transforms.Compose([transforms.Resize([int(2136 * ratio), int(3216 * ratio)]),\n#                                       transforms.CenterCrop([int(2136 * ratio),int(2136 * ratio)]),\n#                                       transforms.RandomHorizontalFlip(p=0.5),\n#                                       transforms.RandomVerticalFlip(p=0.5),\n#                                       transforms.ToTensor()])\n\n# ds = eyeDataset(label_path, image_path, transform_bunch)\n\n# loader = data.DataLoader(\n#     ds,\n#     batch_size=100,\n#     num_workers=2,\n#     shuffle=False\n# )\n\n\n# mean = 0.\n# std = 0.\n# nb_samples = 0.\n# for data1 in loader:\n#     data1 = data1[0]\n#     print(data1.shape)\n#     batch_samples = data1.size(0)\n#     data1 = data1.view(batch_samples, data1.size(1), -1)\n#     mean += data1.mean(2).sum(0)\n#     std += data1.std(2).sum(0)\n#     nb_samples += batch_samples\n\n# mean /= nb_samples\n# std /= nb_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean /= nb_samples\n# std /= nb_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nb_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Alright so I ran it and I got: (tensor([0.5139, 0.2727, 0.0891]), tensor([0.1533, 0.0909, 0.0400]))\n# #Mean, std\n# mean, std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Redoing the transforms, this time with the values of the mean and std to normalize it\n\ntransform_bunch = transforms.Compose([transforms.Resize([int(2136 * ratio), int(3216 * ratio)]),\n                                      transforms.CenterCrop([int(2136 * ratio),int(2136 * ratio)]),\n                                      transforms.RandomHorizontalFlip(p=0.5),\n                                      transforms.RandomVerticalFlip(p=0.5),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize((0.5139, 0.2727, 0.0891), (0.1533, 0.0909, 0.0400))])\n\nds = eyeDataset(label_path, image_path, transform_bunch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I'm not entirely sure if this is right but whatever! \nimg, x = ds[0]\nim = transforms.ToPILImage()(img)\ndisplay(im)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now that we have the dataset, let's put it in the dataloader\nbs = 100\n\ndl = data.DataLoader(ds, bs, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing it out to see if it works --> It does!\n#Also salving one batch for later testings\nxx = 0\nyy = 0\nh = 0\n\nfor x,y in dl:\n    print(x.shape,y.shape)\n    xx = x\n    yy = y\n    break\n#     h += 1\n#     if h == 10:\n#         break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now I'll be making the model! It'll take elements off of nn.Module\nclass cnnmaker(nn.Module):\n    #Initialization will only take in input channels\n    def __init__(self, input_channels):\n        super().__init__()\n        \n        #I looked at what the resnet architechture looked like and just implmented the block types (not the residual part just yet)\n        def block(in_chan):\n            return nn.Sequential(nn.Conv2d(in_chan, in_chan * 2, 3, 2, 1, padding_mode = \"reflect\"),\n                                nn.BatchNorm2d(in_chan*2),\n                                nn.ReLU())\n        \n        self.model = nn.Sequential(block(input_channels),\n                                  block(input_channels*2),\n                                  block(input_channels*4),\n                                  block(input_channels*8),\n                                  block(input_channels*16))\n        \n        self.second_model = nn.Sequential(nn.Linear(4704, 5))\n    #The forward pass when you call it   \n    def forward(self, images):\n        pre_proc = self.model(images)\n#         print(pre_proc.shape)\n        \n        formatted = torch.reshape(pre_proc, (images.shape[0], -1))\n        \n        return self.second_model(formatted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = cnnmaker(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at how many parameters\ntotal = 0\nfor param in model.parameters():\n    if param.requires_grad:\n        total += param.numel()\n\nprint(total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing_outputs = model(xx)\n# print(testing_outputs, yy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing_outputs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_func = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to('cuda')\nloss_func = loss_func.to('cuda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(preds, target):\n    correct = (preds == target).float()\n    accuracy = correct.sum() / len(correct)\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train():\n    model.train()\n    \n    for image, label in dl:\n        image, label = image.type(dtype=torch.cuda.FloatTensor), label.type(dtype=torch.cuda.FloatTensor)\n        optimizer.zero_grad()\n        prediction = model(image)\n        \n        loss = loss_func(prediction, label)\n        \n        loss.backward()\n        \n        optimizer.step()\n    \n    with torch.no_grad():\n        total_iter = 0\n        total_acc = 0\n        for image, label in dl: \n            image, label = image.to('cuda', dtype=torch.float), label.to('cuda', dtype=torch.float)\n            prediction = model(image)\n            total_acc += accuracy(torch.argmax(prediction, dim = 1).float(), torch.argmax(label.float()))\n            total_iter += 1\n            if total_iter == 20:\n                print(total_acc/total_iter)\n                return \"done\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _ in range(4):\n    train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for image, label in dl:\n#     image, label = image.to('cuda'), label.to('cuda')\n    \n#     optimizer.zero_grad()\n#     prediction = model(image)\n\n#     soft_label = F.one_hot(label,num_classes=5).float()\n    \n#     print(prediction, soft_label)\n    \n#     break\n\n#     loss = loss_func(prediction, soft_label)\n\n#     loss.backward()\n\n#     optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}